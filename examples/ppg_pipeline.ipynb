{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__BIOBSS - PPG Pipeline__\n",
    "\n",
    "_This notebook includes guidelines to help using pipeline module for PPG signal processing and feature extraction._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import BIOBSS and other required packages\n",
    "\n",
    "#import biobss\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#import biobss from local to run without installing\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "import biobss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "1. [PPG Sample Data](#sampledata)<br>\n",
    "2. [Peak Detection](#ppg_peak)<br>\n",
    "3. [Create Bio Process Objects](#create_bioprocess)<br>\n",
    "4. [Create Feature Objects](#create_feature)<br>\n",
    "5. [Create Bio Pipeline Object](#create_pipeline)<br>\n",
    "6. [Create Bio Channel Objects](#create_biochannel)<br>\n",
    "7. [Set Pipeline Inputs](#set_input)<br>\n",
    "8. [Add Bio Process Objects to Pipeline](#add_bioprocess)<br>\n",
    "9. [Add Feature Objects to Pipeline](#add_feature)\n",
    "10. [Run Pipeline](#run_pipeline)<br>\n",
    "11. [Extract Features](#extract_features)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __PPG Sample Data__\n",
    "<a id=\"sampledata\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PPG sample data is provided as a csv file in BIOBSS\\sample data. The data file contains 100 PPG segments of 10-seconds length. The sampling rate is 64 Hz for all segments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the sample data\n",
    "data, info = biobss.utils.load_sample_data(data_type='PPG_short')\n",
    "sig = np.asarray(data['PPG'])\n",
    "fs = info['sampling_rate']\n",
    "L = info['signal_length']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Peak Detection__\n",
    "<a id=\"ppg_peak\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BIOBSS provides a peak detection function with different alternatives for the peak detection method. These methods are appropriate for PPG signal, however the parameters should be selected properly if the second peak (diastolic peak) is observable in the signal. The ___peak_detection___ function returns a dictionary including peak locations, peak amplitudes, trough locations and trough amplitudes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For further analysis, a peak control step is required to prevent errors resulting from incorrect peak detection results (missing or dupliciate peaks). The ___peak_control___ function checks the relative locations of peaks and troughs, ensuring only a single peak is located between consecutive troughs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Detect peaks using 'peakdet' method (delta=0.01). Delta parameter should be adjusted related to the amplitude of the signal.\n",
    "info=biobss.preprocess.peak_detection(sig,fs,'peakdet',delta=0.01)\n",
    "\n",
    "locs_peaks=info['Peak_locs']\n",
    "peaks=info['Peaks']\n",
    "locs_onsets=info['Trough_locs']\n",
    "onsets=info['Troughs']\n",
    "\n",
    "#Correct the peak detection results by considering the order of peaks and troughs\n",
    "info=biobss.ppgtools.peak_control(sig=sig, peaks_locs=locs_peaks, troughs_locs=locs_onsets)\n",
    "\n",
    "locs_peaks=info['Peak_locs']\n",
    "peaks=sig[locs_peaks]\n",
    "locs_onsets=info['Trough_locs']\n",
    "onsets=sig[locs_onsets]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='Green'>__Create Bio Process Objects__ </font>\n",
    "<a id=\"create_bioprocess\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any process can be added to the pipeline if the input is a signal and ouput is a signal or collection of signals. The process can be added by passing the method to a Bio_Process constructor. The Bio_Process constructor takes the following arguments:\n",
    "\n",
    "- process_method: The method to be added to the pipeline\n",
    "- inplace: If True, result of the process will modify the input signal. If False, the result of the process will be returned as a new signal and added as a new channel.\n",
    "- prefix: The prefix to be added to the output channel name. If inplace is True, prefix will be ignored.\n",
    "  - If inplace is False, the output channel name will be prefix + input channel name.\n",
    "  - <code> (Default: None) (if inplace = False  and prefix = None, prefix will be set to 'processed') </code>\n",
    "- return_index : If a return_index is set, selected index of the result will be set as result. If return_index is None, the whole result will be set as result. (Default: None)\n",
    "- argmap: this dictionary maps different named arguments. For example, Bio_Channel already have a sampling_rate attribute. If the process method requires sampling rate with a different name, it can be mapped to the Bio_Channel attribute.<code> EXAMPLE : argmap = {'sampling_rate': 'fs'} or argmap = {'sampling_rate': 'sample_rate'} </code>\n",
    "- **kwargs: Keyword arguments to be passed to the process method\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Pipeline processes the given input sequentially. The input is passed from one process to the next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create steps for the pipeline\n",
    "\n",
    "# Filter steps process,return filtered signal for all input signals\n",
    "clean=biobss.pipeline.Bio_Process(\n",
    "    process_method=biobss.ppgtools.filter_ppg, argmap={\"sampling_rate\":\"sampling_rate\"})\n",
    "\n",
    "# Signal normalization step, return normalized signal for all input signals\n",
    "normalize = biobss.pipeline.Bio_Process(\n",
    "    process_method=biobss.preprocess.normalize_signal)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='Green'>__Create Feature Objects__ </font>\n",
    "<a id=\"create_feature\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create feature extraction steps\n",
    "\n",
    "# Extract time domain features. input_signals dictionary contains feature prefixes as keys and input signals as values\n",
    "# For example rms of EDA_Tonic is extracted as Tonic_rms in this case\n",
    "segment_features = biobss.pipeline.Feature(name=\"time_features\", function=biobss.ppgtools.from_segment, input_signals={'PPG_Raw':'PPG_Raw'}, feature_types=['Freq','Time','Stat'], sampling_rate=fs, argmap={'sampling_rate':'sampling_rate'})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Create Bio Pipeline Object__\n",
    "<a id=\"create_pipeline\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BIOBSS Pipeline is created\n",
    "# windowed_process=True means that the pipeline will process the signal in windows\n",
    "# window_size=60 means that the pipeline will process 60 seconds of data at a time\n",
    "# step_size=20 means signal will be shifted by 20 seconds for each window\n",
    "pipeline = biobss.pipeline.Bio_Pipeline(windowed_process=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Create Bio Channel Objects__\n",
    "<a id=\"create_biochannel\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input is created as Bio_Channel object\n",
    "# Bio_Channel object requires following parameters: name, signal, sampling_rate\n",
    "# Optionally timestamp can be provided\n",
    "# If timestamp is provided, resolution of the timestamp should be provided. Possible resolutions are:'min', 's', 'ms', 'us'\n",
    "# If timestamp is not provided, timestamp will be created using sampling_rate. Default resolution is 's'\n",
    "bio_channel=biobss.pipeline.Bio_Channel(signal=sig,name=\"PPG_Raw\",sampling_rate=fs)\n",
    "\n",
    "# a Bio Channel object is created with sample data, with the name EDA_Raw and sampling rate 700\n",
    "# timestamp is not provided, so timestamp will be created using sampling_rate\n",
    "\n",
    "# Simply bio_channel the channel object will print the channel properties\n",
    "bio_channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bio_channel.channel accesses the data in the channel object\n",
    "bio_channel.channel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Set Pipeline Inputs__\n",
    "<a id=\"set_input\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline input can be set from an array, a dataframe, pandas series, list, Bio_Channel, Bio_Data\n",
    "\n",
    "# In this case, the pipeline input is set with an array\n",
    "pipeline.set_input(sig,sampling_rate=fs,name='PPG_Raw')\n",
    "\n",
    "# Alternatively, the pipeline input can be set with a Bio_Channel object, in this case the name and sampling rate are not required as they are already provided in the Bio_Channel object\n",
    "pipeline.set_input(bio_channel)\n",
    "\n",
    "pipeline.input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Add Bio Process Objects to Pipeline__\n",
    "<a id=\"add_bioprocess\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline steps are added to the pipeline sequentially, the order of the steps is important as the output of one step is the input of the next step\n",
    "# These steps will be processed in the order they are added to the pipeline\n",
    "pipeline.preprocess_queue.add_process(clean)\n",
    "pipeline.preprocess_queue.add_process(normalize)\n",
    "\n",
    "\n",
    "# Currently all the steps are added to the preprocess_queue\n",
    "# After the preprocess_queue is processed, data will be segmented into windows (if windowed_process=True)\n",
    "# For running process in windows, the process_queue is used\n",
    "\n",
    "## This structure will change in the future"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Add Feature Objects to Pipeline__\n",
    "<a id=\"add_feature\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features are added to the pipeline\n",
    "pipeline.add_feature_step(segment_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Represetation of the pipeline\n",
    "pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Run Pipeline__\n",
    "<a id=\"run_pipeline\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline is run, this will process the input data in the pipeline\n",
    "pipeline.run_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Represetation of the pipeline data after running\n",
    "pipeline.data\n",
    "\n",
    "# Data is cleaned\n",
    "# Data is normalized\n",
    "# Data is decomposed into tonic and phasic components, these componenets are added to the pipeline data as EDA_Tonic and EDA_Phasic channels\n",
    "# Lastly, data is resampled to 350Hz (All channels are resampled to 350Hz) (target_sample_rate=350)\n",
    "# Data is segmented into windows of 60 seconds with a step size of 20 seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Extract Features__\n",
    "<a id=\"extract_features\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical and signal features are extracted from the EDA_Tonic, EDA_Phasic and EDA_Raw channels\n",
    "# Supplied prefix is added to the feature name\n",
    "pipeline.extract_features()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Represetation of the pipeline features after extraction\n",
    "# Each row is a feature vector for a window\n",
    "# index is the timestamp of the window (this can be selected as start, end or center of the window)\n",
    "pipeline.features"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('biolib')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3a4bcfb23c7e6ad66c655087280fa9f4d0273121ae7909f7735d1e02563a2438"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
